<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Sam — Projects</title>
<style>
  :root{
    --bg-1:#020617;
    --bg-2:#06122a;
    --neon-1:#00e5ff;
    --neon-2:#0f6fff;
    --muted:#9aa7b6;
  }
  html, body {
    margin: 0; padding: 0; height: 100%;
    font-family: Inter, sans-serif;
    background: linear-gradient(180deg, var(--bg-1), var(--bg-2));
    color: white;
    overflow-x: hidden;
  }
  body.modal-open {
    overflow: hidden;
  }

  /* NAVBAR */
  nav.tabs {
    position: fixed;
    top: 1rem;
    right: 2rem;
    display: flex;
    gap: 1.5rem;
    z-index: 100;
  }
  nav.tabs a {
    color: var(--neon-1);
    text-decoration: none;
    font-weight: 600;
    font-size: 1rem;
    padding: 0.3rem 0.6rem;
    border-radius: 6px;
    border: 1.5px solid var(--neon-1);
    transition: background 0.3s, color 0.3s;
  }
  nav.tabs a:hover,
  nav.tabs a:focus {
    background: var(--neon-1);
    color: #04122a;
    box-shadow: 0 0 10px var(--neon-1);
    outline: none;
  }
  nav.tabs a.active {
    background: var(--neon-1);
    color: #04122a;
  }

  .hamburger {
    display: none;
    position: fixed;
    top: 1rem;
    right: 2rem;
    width: 30px;
    height: 22px;
    cursor: pointer;
    z-index: 110;
    flex-direction: column;
    justify-content: space-between;
  }
  .hamburger span {
    display: block;
    height: 4px;
    background: var(--neon-1);
    border-radius: 2px;
  }

  .sidebar {
    position: fixed;
    top: 0;
    right: 0;
    width: 250px;
    height: 100vh;
    background: var(--bg-2);
    padding: 3rem 1rem 1rem 1rem;
    display: none;
    flex-direction: column;
    gap: 1rem;
    z-index: 105;
  }
  .sidebar.show {
    display: flex;
  }
  .sidebar a {
    color: var(--neon-1);
    text-decoration: none;
    font-weight: 600;
    padding: 0.5rem 0.9rem;
    border-radius: 6px;
    border: 1.5px solid var(--neon-1);
    transition: background 0.3s, color 0.3s;
  }
  .sidebar a:hover,
  .sidebar a:focus {
    background: var(--neon-1);
    color: #04122a;
    outline: none;
  }
  .sidebar a.active {
    background: var(--neon-1);
    color: #04122a;
  }

  /* MAIN */
  main#projects-main {
    position: relative;
    min-height: 100vh;
    padding: 6rem 1rem 4rem;
    display: flex;
    justify-content: center;
    align-items: center;
    overflow: hidden;
  }

  /* The dotted horizontal center line */
.belt-line {
  position: absolute;
  top: 50%;
  left: 0;
  width: 100%;
  height: 4px; /* thicker */
  background-image: radial-gradient(var(--neon-1) 3px, transparent 3px); /* bigger dots */
  background-size: 16px 4px;
  transform: translateY(-50%);
  z-index: 0;
}


  /* Conveyor belt container */
.belt-container {
  width: 100%;
  overflow-x: hidden; /* only hide horizontally */
  overflow-y: visible; /* allow vertical overflow */
  position: relative;
  z-index: 1;
  padding-top: 4rem; /* extra space */
  padding-bottom: 4rem;
}

  /* Scrolling track */
.belt-scroll {
  display: flex;
  gap: 4rem;
  padding: 2rem 0;
  will-change: transform;
  /* animation removed */
  touch-action: pan-y; /* allow vertical scroll normally but horizontal drag handled manually */
  cursor: grab;
  user-select: none;
}
.belt-scroll:active {
  cursor: grabbing;
}

  .belt-container:hover .belt-scroll {
    animation-play-state: paused;
    cursor: pointer;
  }

  @keyframes scroll-left {
    0% { transform: translateX(0); }
    100% { transform: translateX(-50%); }
  }

  /* Project nodes */
  .project-node {
    position: relative;
    flex: 0 0 220px;
    background: var(--bg-2);
    border: 1px solid var(--neon-1);
    border-radius: 12px;
    box-shadow: 0 0 12px var(--neon-1);
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    cursor: pointer;
    padding: 1rem 1rem 1.2rem;
    text-align: center;
    color: white;
    font-weight: 600;
    user-select: none;
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    z-index: 1;
  }
  .project-node:hover,
  .project-node:focus-visible {
    outline: none;
    transform: scale(1.1);
    box-shadow: 0 0 25px var(--neon-2);
  }
  .project-node h3 {
    margin: 0 0 0.3rem;
    font-size: 1.1rem;
    line-height: 1.2;
  }
  .project-node p {
    margin: 0;
    font-size: 0.85rem;
    color: var(--neon-1);
    opacity: 0.8;
    line-height: 1.1;
  }

  /* Alternate vertical positions */
.project-node:nth-child(odd) {
  margin-bottom: 3rem;
  margin-top: 0;
  transform: translateY(-5rem);
}

.project-node:nth-child(even) {
  margin-top: 3rem;
  margin-bottom: 0;
  transform: translateY(3rem);
}


  /* MODAL */
  .modal {
    position: fixed;
    top: 0; left: 0; width: 100vw; height: 100vh;
    background: rgba(2,6,23,0.9);
    backdrop-filter: blur(5px);
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 1rem;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s ease, visibility 0.3s ease;
  }
  .modal.show {
    opacity: 1;
    visibility: visible;
  }
  .modal-content {
    background: var(--bg-2);
    border: 1px solid var(--neon-2);
    box-shadow: 0 0 30px rgba(15,111,255,0.3);
    border-radius: 16px;
    max-width: 800px;
    width: 90vw;
    max-height: 90vh;
    display: flex;
    flex-direction: column;
    position: relative;
    overflow: hidden;
  }
  .close-modal {
    position: absolute;
    top: 1rem;
    right: 1rem;
    background: none;
    border: none;
    font-size: 2rem;
    color: var(--muted);
    cursor: pointer;
    line-height: 1;
    transition: color 0.3s ease;
    z-index: 10;
  }
  .close-modal:hover,
  .close-modal:focus {
    color: white;
    outline: none;
  }
  .modal-body {
    padding: 2rem;
    overflow-y: auto;
    color: var(--muted);
  }
  .modal-body img {
    width: 100%;
    max-height: 550px;
    object-fit: cover;
    border-radius: 8px;
    margin-bottom: 1.5rem;
    background-color: var(--bg-1);
  }
  .modal-body h2 {
    font-size: clamp(1.5rem, 4vw, 2.25rem);
    background: linear-gradient(90deg, var(--neon-2), var(--neon-1));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    margin: 0 0 1rem;
  }
  .modal-body p {
    line-height: 1.6;
    margin-bottom: 1.5rem;
  }
  .modal-github-link {
    display: inline-flex;
    align-items: center;
    gap: 0.6rem;
    padding: 0.6rem 1.2rem;
    border-radius: 10px;
    border: 1.5px solid var(--neon-1);
    background: none;
    text-decoration: none;
    color: var(--neon-1);
    font-weight: 700;
    transition: all 0.2s ease;
  }
  .modal-github-link:hover,
  .modal-github-link:focus {
    transform: translateY(-3px);
    box-shadow: 0 0 18px rgba(0,229,255,0.25);
    background: rgba(0,229,255,0.1);
    outline: none;
  }
  .icon-svg {
    width: 20px;
    height: 20px;
    fill: currentColor;
  }

  /* RESPONSIVE */
  @media(max-width:900px) {
    .page-toggle {
    width: 90%;             /* container takes most of screen */
    left: 50%;
    transform: translateX(-50%);
    display: flex;          /* keep side by side */
  }

  .page-toggle .toggle-btn {
    flex: 1 1 0;            /* make all buttons equal width */
    text-align: center;
    padding: 0.6rem 0;      /* reduce horizontal padding, width is now flex-based */
    font-size: 1rem;
  }
    nav.tabs { display: none; }
    .hamburger { display: flex; }
    main#projects-main {
      padding: 5rem 1rem 4rem;
    }
    .belt-scroll {
      gap: 2rem;
      padding: 1rem 0;
    }
   .project-node {
  flex: 0 0 160px;
  font-size: 0.85rem;       /* slightly smaller text */
  padding: 0.4rem 0.6rem;   /* less padding inside box */
}

.project-node h3 {
  font-size: 0.95rem;       /* slightly smaller heading */
}
    
/* Odd nodes (above belt) */
.project-node:nth-child(odd) {
  margin-bottom: 4rem; /* pushes node further from belt */
}

.project-node:nth-child(odd)::before {
  top: 100%;
  left: 50%;
  transform: translateX(-50%);
  height: 0.8rem;
}

/* Even nodes (below belt) */
.project-node:nth-child(even) {
  margin-top: 5rem; /* pushes node further from belt */
}

.project-node:nth-child(even)::before {
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  height: 0.8rem;
}



  }

.page-toggle {
  position: absolute;
  top: 4.5rem; /* just below navbar */
  left: 50%;
  transform: translateX(-50%);
  display: flex;
  border: 2px solid var(--neon-1);
  border-radius: 8px;
  overflow: hidden;
  z-index: 5;
}

.page-toggle .toggle-btn {
  padding: 0.5rem 1.2rem;
  background: transparent;
  color: var(--neon-1);
  text-decoration: none;
  font-weight: 600;
  transition: all 0.3s ease;
}

.page-toggle .toggle-btn:hover {
  background: rgba(0, 229, 255, 0.15);
}

.page-toggle .toggle-btn.active {
  background: var(--neon-1);
  color: var(--bg-2);
}

/* Base connector line style */
.project-node::before {
  content: "";
  position: absolute;
  left: 50%;
  width: 2px; /* thickness of the vertical line */
  background: var(--neon-1);
  transform: translateX(-50%);
  z-index: -1; /* behind the node */
}

/* For nodes ABOVE the belt (odd ones) - line goes DOWN to belt */
.project-node:nth-child(odd)::before {
  top: 100%; /* start from bottom of node */
  height: calc(1.8rem); /* length to reach belt from top-offset nodes */
}

/* For nodes BELOW the belt (even ones) - line goes UP to belt */
.project-node:nth-child(even)::before {
  bottom: 100%; /* start from top of node */
  height: calc(1.8rem); /* length to reach belt from bottom-offset nodes */
}



</style>
</head>
<body>

<nav class="tabs" aria-label="Main navigation">
  <a href="index.html">Home</a>
  <a href="skills.html">Skills</a>
  <a href="experience.html">Experience</a>
  <a href="projects.html" class="active">Projects</a>
  <!--<a href="pcbs.html">PCBs</a>-->
  <a href="about.html">About</a>
  <a href="resume.html">Resume</a>
  <a href="contact.html">Contact</a>
</nav>

<div class="hamburger" aria-label="Open navigation menu" role="button" tabindex="0" aria-expanded="false" aria-controls="mobile-menu">
  <span></span><span></span><span></span>
</div>

<nav id="mobile-menu" class="sidebar" aria-label="Mobile navigation" hidden>
  <a href="index.html">Home</a>
  <a href="skills.html">Skills</a>
  <a href="experience.html">Experience</a>
  <a href="projects.html" class="active">Projects</a>
  <!--<a href="pcbs.html">PCBs</a>-->
  <a href="about.html">About</a>
  <a href="resume.html">Resume</a>
  <a href="contact.html">Contact</a>
</nav>

<main id="projects-main" aria-label="Project list">

  <div class="page-toggle">
  <a href="projects.html" class="toggle-btn active">Hardware & Software</a>
  <a href="pcbs.html" class="toggle-btn">PCB Gallery</a>
</div>


  <div class="belt-line" aria-hidden="true"></div>
  <div class="belt-container" aria-live="polite">
    <div class="belt-scroll" aria-label="Scrolling project nodes">
      <!-- Project nodes duplicated twice for seamless infinite scroll -->

      <div class="project-node" tabindex="0"
        data-title="INSITE Bioprinter Project"
        data-img="assets\INSITE.png"
        data-desc="Awarded the highly competitive First Year Summer Research Fellowship (1 of 15 selected from over 1,000 first-year engineering students), supervised by Professor Axel Guenther. Redesigning the INSITE handheld bioprinter to improve reliability and clinical usability. Enhancing motion control systems and firmware for greater printing precision and real-time responsiveness. Prototyping intuitive control interfaces and iterating designs with PhD mentors to enhance surgical experience.">
        <h3>INSITE Bioprinter</h3>
        <p>Summer Research Fellowship</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="ZenVision"
        data-img="assets\ZenVision Glasses.png"
        data-desc="ZenVision is a wearable smart glasses system designed to support children with ADHD who struggle with impulsive verbal outbursts in the classroom. Built for a 9-year-old child, Megan, introduced to us by our clients, the system discreetly detects when she speaks out loudly—often a response to anxiety—and gently delivers visual reminders on the lens to help with self-regulation, without singling her out. The system uses an ESP32 microcontroller and MAX4466 microphone to monitor sound in real-time. A custom CNN (built with TensorFlow and trained on MFCCs) confirms whether the voice is Megan’s before triggering the prompt—achieving an average response time of just 1.93 seconds. A companion web dashboard (HTML/CSS/JS + Plotly.js) logs these events to a real-time database, enabling teachers and parents to track behavioral patterns and progress over time. Selected as a Top 20 Project (out of 187 teams) in the UofT ESP (APS 112) Showcase."
        data-github="https://github.com/SemontiiMandal/ZenVision">
        <h3>ZenVision</h3>
        <p>ADHD Support Glasses</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Auracle"
        data-img="assets\Auracle Glasses and Gloves.jpg"
        data-desc="Auracle is a wearable system that combines glasses and gloves for voice- and gesture-based music generation and control. The glasses are equipped with a voice recording module that captures user commands, which are then converted to text using Hugging Face’s speech-to-text model. This text prompt is passed to Hugging Face’s AudioLDM 2, a generative AI model, to create a unique music track tailored to the user’s request. Sentiment analysis is performed on the text prompt using TextBlob to determine the emotional tone, and this data is logged into Firebase’s Realtime Database and visualized on a Streamlit dashboard to track the user’s mood history. The gloves, equipped with accelerometer and gyroscope sensors, allow the user to control music functions such as skipping tracks, adjusting volume, and stopping the music through simple hand gestures. A full-stack application built with Streamlit manages user profiles, music generation history, and mood tracking."
        data-github="https://github.com/SemontiiMandal/Auracle">
        <h3>Auracle</h3>
        <p>Voice and Gesture-Controlled Music Generation System</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Wildfire Response"
        data-img="assets\UTEK.jpg"
        data-desc="Developed an automated response system in Python to rescue citizens in distress during wildfires and manage communication between users and dispatch centers. Integrated Twilio and Google Sheets APIs (gspread) to collect user details—such as injury severity, age, and location—via SMS and store them in a centralized data repository. Designed a priority-based dispatch algorithm that deploys rescuers by assessing priority according to Canada’s Ministry of Health’s Triage and Acuity Scale (CTAS) guidelines, ensuring efficient and accurate resource allocation."
        data-github="https://github.com/SemontiiMandal/Automated-Emergency-Response-System-for-Wildfires">
        <h3>Wildfire Response</h3>
        <p>Automated Emergency Response System</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Wearable Breathing Biofeedback Device"
        data-img="assets\ut_biome_logo.jpg"
        data-desc="We are collaborating with Prof. Christopher Bouwmeester (UofT IBME & ISTEP) to design a wearable biofeedback device that monitors real-time breathing patterns. The device aims to assist users in learning deep breathing techniques, such as diaphragmatic breathing. Using Machine Learning algorithms, it will analyze breathing patterns to detect potential diseases. The device incorporates an Arduino Nano, stretch sensors, FFT analysis, and buzzer feedback for real-time guidance and monitoring.">
        <h3>Breathing Biofeedback</h3>
        <p>UTBIOME (UofT Biomedical Engineering) Design Team</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Kidney Stone Detection"
        data-img="assets\ks.png"
        data-desc="The Kidney Stone Detection System is a machine learning-based solution designed to predict kidney stones by analyzing the physical characteristics of urine. Using supervised learning classifiers, such as Logistic Regression, Decision Tree, and Support Vector Machine (SVM), the system detects potential abnormalities, offering a non-invasive and cost-effective alternative to imaging techniques like CT scans. Trained on a dataset of urine characteristics, the system achieved over 80% accuracy and an F1 score above 85%."
        data-github="https://github.com/SemontiiMandal/Research-Paper-Kidney-Stones-JEI">
        <h3>Kidney Stone Detection</h3>
        <p>Published ML Research</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Plant Disease Detector"
        data-img="assets\leaves.jpeg"
        data-desc="The Plant Disease Detection project uses deep learning to identify plant diseases from images. It leverages the ResNet50 model, a pre-trained Convolutional Neural Network (CNN), fine-tuned to classify images into 38 categories of plant diseases. The model is trained on an augmented dataset, incorporating techniques like zoom, shift, and flipping to prevent overfitting. With Python, TensorFlow, and Keras, the project achieves over 96.5% classification accuracy."
        data-github="https://github.com/SemontiiMandal/Plant-Disease-Detection">
        <h3>Plant Disease Detecting Algorithm</h3>
        <p>Deep Learning / CNN</p>
      </div>


      <div class="project-node" tabindex="0"
        data-title="Malicious URL Detector"
        data-img="assets\cs.jpg"
        data-desc="The Malicious URL Detector is a machine learning project designed to classify URLs as either malicious or benign, enhancing cybersecurity. Using natural language processing (NLP) techniques, the model tokenizes URLs into features and applies algorithms like Logistic Regression and Support Vector Machines (SVM) for classification. The project utilizes Python and libraries such as Pandas, NumPy, and Scikit-learn for data manipulation, feature extraction, and model evaluation. By analyzing URL patterns, it aims to identify harmful links and strengthen online security."
        data-github="https://github.com/SemontiiMandal/Malicious-URL-Detector">
        <h3>Malicious URL Detector</h3>
        <p>New York Academy of Sciences (NYAS) Junior Academy Global Finalist, Spring 2023 (Cybersecurity)</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="MNIST Digits"
        data-img="assets\MNIST_dataset_example.png"
        data-desc="The MNIST Handwritten Digits Recognition project uses deep learning to classify handwritten digits from the MNIST dataset. The model employs a neural network with dense layers and ReLU activation, trained using TensorFlow. It is designed to accurately recognize digits and can be extended for custom images. The network achieves 98.6% accuracy on the MNIST test set, with a three-layer architecture that includes flattening the input image, two hidden dense layers, and a final output layer with softmax activation."
        data-github="https://github.com/SemontiiMandal/MNIST-Classification">
        <h3>MNIST Digits</h3>
        <p>ML Classification</p>
      </div>

      <!-- Duplicate for infinite scroll -->

            <div class="project-node" tabindex="0"
        data-title="INSITE Bioprinter Project"
        data-img="assets\INSITE.png"
        data-desc="Awarded the highly competitive First Year Summer Research Fellowship (1 of 15 selected from over 1,000 first-year engineering students), supervised by Professor Axel Guenther. Redesigning the INSITE handheld bioprinter to improve reliability and clinical usability. Enhancing motion control systems and firmware for greater printing precision and real-time responsiveness. Prototyping intuitive control interfaces and iterating designs with PhD mentors to enhance surgical experience.">
        <h3>INSITE Bioprinter</h3>
        <p>Summer Research Fellowship</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="ZenVision"
        data-img="assets\ZenVision Glasses.png"
        data-desc="ZenVision is a wearable smart glasses system designed to support children with ADHD who struggle with impulsive verbal outbursts in the classroom. Built for a 9-year-old child, Megan, introduced to us by our clients, the system discreetly detects when she speaks out loudly—often a response to anxiety—and gently delivers visual reminders on the lens to help with self-regulation, without singling her out. The system uses an ESP32 microcontroller and MAX4466 microphone to monitor sound in real-time. A custom CNN (built with TensorFlow and trained on MFCCs) confirms whether the voice is Megan’s before triggering the prompt—achieving an average response time of just 1.93 seconds. A companion web dashboard (HTML/CSS/JS + Plotly.js) logs these events to a real-time database, enabling teachers and parents to track behavioral patterns and progress over time. Selected as a Top 20 Project (out of 187 teams) in the UofT ESP (APS 112) Showcase."
        data-github="https://github.com/SemontiiMandal/ZenVision">
        <h3>ZenVision</h3>
        <p>ADHD Support Glasses</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Auracle"
        data-img="assets\Auracle Glasses and Gloves.jpg"
        data-desc="Auracle is a wearable system that combines glasses and gloves for voice- and gesture-based music generation and control. The glasses are equipped with a voice recording module that captures user commands, which are then converted to text using Hugging Face’s speech-to-text model. This text prompt is passed to Hugging Face’s AudioLDM 2, a generative AI model, to create a unique music track tailored to the user’s request. Sentiment analysis is performed on the text prompt using TextBlob to determine the emotional tone, and this data is logged into Firebase’s Realtime Database and visualized on a Streamlit dashboard to track the user’s mood history. The gloves, equipped with accelerometer and gyroscope sensors, allow the user to control music functions such as skipping tracks, adjusting volume, and stopping the music through simple hand gestures. A full-stack application built with Streamlit manages user profiles, music generation history, and mood tracking."
        data-github="https://github.com/SemontiiMandal/Auracle">
        <h3>Auracle</h3>
        <p>Voice and Gesture-Controlled Music Generation System</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Wildfire Response"
        data-img="assets\UTEK.jpg"
        data-desc="Developed an automated response system in Python to rescue citizens in distress during wildfires and manage communication between users and dispatch centers. Integrated Twilio and Google Sheets APIs (gspread) to collect user details—such as injury severity, age, and location—via SMS and store them in a centralized data repository. Designed a priority-based dispatch algorithm that deploys rescuers by assessing priority according to Canada’s Ministry of Health’s Triage and Acuity Scale (CTAS) guidelines, ensuring efficient and accurate resource allocation."
        data-github="https://github.com/SemontiiMandal/Automated-Emergency-Response-System-for-Wildfires">
        <h3>Wildfire Response</h3>
        <p>Automated Emergency Response System</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Wearable Breathing Biofeedback Device"
        data-img="assets\ut_biome_logo.jpg"
        data-desc="We are collaborating with Prof. Christopher Bouwmeester (UofT IBME & ISTEP) to design a wearable biofeedback device that monitors real-time breathing patterns. The device aims to assist users in learning deep breathing techniques, such as diaphragmatic breathing. Using Machine Learning algorithms, it will analyze breathing patterns to detect potential diseases. The device incorporates an Arduino Nano, stretch sensors, FFT analysis, and buzzer feedback for real-time guidance and monitoring.">
        <h3>Breathing Biofeedback</h3>
        <p>UTBIOME (UofT Biomedical Engineering) Design Team</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Kidney Stone Detection"
        data-img="assets\ks.png"
        data-desc="The Kidney Stone Detection System is a machine learning-based solution designed to predict kidney stones by analyzing the physical characteristics of urine. Using supervised learning classifiers, such as Logistic Regression, Decision Tree, and Support Vector Machine (SVM), the system detects potential abnormalities, offering a non-invasive and cost-effective alternative to imaging techniques like CT scans. Trained on a dataset of urine characteristics, the system achieved over 80% accuracy and an F1 score above 85%."
        data-github="https://github.com/SemontiiMandal/Research-Paper-Kidney-Stones-JEI">
        <h3>Kidney Stone Detection</h3>
        <p>Published ML Research</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="Plant Disease Detector"
        data-img="assets\leaves.jpeg"
        data-desc="The Plant Disease Detection project uses deep learning to identify plant diseases from images. It leverages the ResNet50 model, a pre-trained Convolutional Neural Network (CNN), fine-tuned to classify images into 38 categories of plant diseases. The model is trained on an augmented dataset, incorporating techniques like zoom, shift, and flipping to prevent overfitting. With Python, TensorFlow, and Keras, the project achieves over 96.5% classification accuracy."
        data-github="https://github.com/SemontiiMandal/Plant-Disease-Detection">
        <h3>Plant Disease Detecting Algorithm</h3>
        <p>Deep Learning / CNN</p>
      </div>


      <div class="project-node" tabindex="0"
        data-title="Malicious URL Detector"
        data-img="assets\cs.jpg"
        data-desc="The Malicious URL Detector is a machine learning project designed to classify URLs as either malicious or benign, enhancing cybersecurity. Using natural language processing (NLP) techniques, the model tokenizes URLs into features and applies algorithms like Logistic Regression and Support Vector Machines (SVM) for classification. The project utilizes Python and libraries such as Pandas, NumPy, and Scikit-learn for data manipulation, feature extraction, and model evaluation. By analyzing URL patterns, it aims to identify harmful links and strengthen online security."
        data-github="https://github.com/SemontiiMandal/Malicious-URL-Detector">
        <h3>Malicious URL Detector</h3>
        <p>New York Academy of Sciences (NYAS) Junior Academy Global Finalist, Spring 2023 (Cybersecurity)</p>
      </div>

      <div class="project-node" tabindex="0"
        data-title="MNIST Digits"
        data-img="assets\MNIST_dataset_example.png"
        data-desc="The MNIST Handwritten Digits Recognition project uses deep learning to classify handwritten digits from the MNIST dataset. The model employs a neural network with dense layers and ReLU activation, trained using TensorFlow. It is designed to accurately recognize digits and can be extended for custom images. The network achieves 98.6% accuracy on the MNIST test set, with a three-layer architecture that includes flattening the input image, two hidden dense layers, and a final output layer with softmax activation."
        data-github="https://github.com/SemontiiMandal/MNIST-Classification">
        <h3>MNIST Digits</h3>
        <p>ML Classification</p>
      </div>
    </div>
  </div>
</main>

<!-- MODAL -->
<div id="project-modal" class="modal" role="dialog" aria-modal="true" aria-labelledby="modal-title" aria-describedby="modal-desc" hidden>
  <div class="modal-content">
    <button class="close-modal" aria-label="Close project details">&times;</button>
    <div class="modal-body">
      <img id="modal-img" src="" alt="" />
      <h2 id="modal-title"></h2>
      <p id="modal-desc"></p>
      <a id="modal-github" href="#" target="_blank" rel="noopener" class="modal-github-link" aria-label="View GitHub repository" style="display:none;">
        <svg class="icon-svg" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
          <path d="M12 .5C5.73.5.75 5.48.75 11.74c0 4.9 3.18 9.06 7.58 10.53.55.1.75-.24.75-.53 0-.26-.01-1.13-.02-2.05-3.08.67-3.73-1.49-3.73-1.49-.5-1.28-1.22-1.62-1.22-1.62-.99-.68.08-.67.08-.67 1.1.08 1.68 1.13 1.68 1.13.97 1.66 2.55 1.18 3.17.9.1-.7.38-1.18.69-1.45-2.46-.28-5.05-1.23-5.05-5.47 0-1.21.43-2.2 1.13-2.98-.12-.28-.49-1.4.11-2.92 0 0 .92-.3 3.02 1.13.88-.25 1.82-.38 2.75-.38.93 0 1.87.13 2.75.38 2.1-1.43 3.02-1.13 3.02-1.13.6 1.52.23 2.64.11 2.92.7.78 1.13 1.77 1.13 2.98 0 4.24-2.6 5.19-5.07 5.46.39.34.73 1.02.73 2.06 0 1.49-.01 2.7-.01 3.07 0 .29.2.64.76.53C20.07 20.8 23.25 16.64 23.25 11.74 23.25 5.48 18.27.5 12 .5z"/>
        </svg>
        GitHub
      </a>
    </div>
  </div>
</div>

<footer style="text-align:center;padding:1.5rem 0;color:var(--muted);font-size:0.85rem;">
  &copy; Sam — ECE @ UofT
</footer>

<script>
  // Hamburger toggle (same behavior as other pages)
  const hamburger = document.querySelector('.hamburger');
  const sidebar = document.getElementById('mobile-menu');

  function toggleMenu(){
    const isOpen = sidebar.classList.toggle('show');
    if(isOpen) sidebar.removeAttribute('hidden'); else sidebar.setAttribute('hidden','');
    hamburger.setAttribute('aria-expanded', isOpen);
  }
  hamburger.addEventListener('click', toggleMenu);
  hamburger.addEventListener('keydown', (e) => {
    if(e.key === 'Enter' || e.key === ' ') { e.preventDefault(); toggleMenu(); }
  });

  sidebar.querySelectorAll('a').forEach(a => a.addEventListener('click', () => {
    sidebar.classList.remove('show'); sidebar.setAttribute('hidden',''); hamburger.setAttribute('aria-expanded', false);
  }));

  // Modal functionality
  const modal = document.getElementById('project-modal');
  const modalTitle = modal.querySelector('#modal-title');
  const modalDesc = modal.querySelector('#modal-desc');
  const modalImg = modal.querySelector('#modal-img');
  const modalGitHub = modal.querySelector('#modal-github');
  const closeBtn = modal.querySelector('.close-modal');

  // Open modal on project node click or Enter key
  document.querySelectorAll('.project-node').forEach(node => {
    node.addEventListener('click', () => openModal(node));
    node.addEventListener('keydown', e => {
      if(e.key === 'Enter' || e.key === ' ') {
        e.preventDefault();
        openModal(node);
      }
    });
  });

  function openModal(node) {
    const title = node.getAttribute('data-title');
    const desc = node.getAttribute('data-desc');
    const img = node.getAttribute('data-img');
    const github = node.getAttribute('data-github');

    modalTitle.textContent = title;
    modalDesc.textContent = desc;
    modalImg.src = img;
    modalImg.alt = title + " screenshot";

    if(github) {
      modalGitHub.href = github;
      modalGitHub.style.display = 'inline-flex';
    } else {
      modalGitHub.style.display = 'none';
    }

    modal.hidden = false;
    modal.classList.add('show');
    document.body.classList.add('modal-open');
    closeBtn.focus();
  }

  // Close modal
  closeBtn.addEventListener('click', closeModal);
  modal.addEventListener('click', e => {
    if (e.target === modal) closeModal();
  });

  // Close modal on Escape key
  window.addEventListener('keydown', e => {
    if(e.key === 'Escape' && !modal.hidden) {
      closeModal();
    }
  });

  function closeModal() {
    modal.classList.remove('show');
    modal.hidden = true;
    document.body.classList.remove('modal-open');
  }

(() => {
  const beltScroll = document.querySelector('.belt-scroll');
  const speed = 0.5; // pixels per frame (adjust for scroll speed)
  let position = 0; // current translateX
  let isDragging = false;
  let dragStartX = 0;
  let dragCurrentX = 0;
  let lastTimestamp = null;

  // Total scrollable width before looping back (half of content because duplicated)
  const totalScrollWidth = beltScroll.scrollWidth / 2;

  function step(timestamp) {
    if (!lastTimestamp) lastTimestamp = timestamp;
    const delta = timestamp - lastTimestamp;
    lastTimestamp = timestamp;

    if (!isDragging) {
      // Move left by speed * time delta (convert ms to seconds)
      position -= speed * (delta / 16); // assuming 60fps baseline ~16ms/frame
      if (position <= -totalScrollWidth) {
        // loop back
        position += totalScrollWidth;
      }
      beltScroll.style.transform = `translateX(${position}px)`;
    } else {
      // When dragging, translateX is controlled by drag offset
      const dragOffset = dragCurrentX - dragStartX;
      let newPos = position + dragOffset;

      // Clamp within 0 to -totalScrollWidth range with looping
      if (newPos > 0) newPos -= totalScrollWidth;
      else if (newPos <= -totalScrollWidth) newPos += totalScrollWidth;

      beltScroll.style.transform = `translateX(${newPos}px)`;
    }
    requestAnimationFrame(step);
  }

  // Touch and mouse drag handlers
  function onDragStart(e) {
    isDragging = true;
    dragStartX = e.type.includes('touch') ? e.touches[0].clientX : e.clientX;
    dragCurrentX = dragStartX;
    beltScroll.style.cursor = 'grabbing';
  }

  function onDragMove(e) {
    if (!isDragging) return;
    dragCurrentX = e.type.includes('touch') ? e.touches[0].clientX : e.clientX;
  }

  function onDragEnd(e) {
    if (!isDragging) return;
    isDragging = false;
    // Update position by the drag distance
    const dragDistance = dragCurrentX - dragStartX;
    position += dragDistance;
    // Normalize position within looping range
    if (position > 0) position -= totalScrollWidth;
    else if (position <= -totalScrollWidth) position += totalScrollWidth;

    beltScroll.style.cursor = 'grab';
  }

  beltScroll.addEventListener('mousedown', onDragStart);
  beltScroll.addEventListener('touchstart', onDragStart, {passive:true});
  window.addEventListener('mousemove', onDragMove);
  window.addEventListener('touchmove', onDragMove, {passive:true});
  window.addEventListener('mouseup', onDragEnd);
  window.addEventListener('touchend', onDragEnd);

  requestAnimationFrame(step);
})();

</script>

</body>
</html>
